\chapter{Methodology} \label{ch:3}

One of the main aims of the thesis was to develop a pipeline for statistical analysis of available protein structure annotations (hereinafter referred to as features), and to prepare this pipeline for adding user-defined features.

The process starts with downloading FASTA and PDB files for input proteins from databases. Residue-level mappings are downloaded as well, to allow cross-referencing the protein tertiary structure with the sequence annotations. After that, values for all features are computed or downloaded and assigned to each residue. Residues are labeled as binding or non-binding according to the ligands defined in the PDB file. As the next step, we perform statistical analysis of the features, using computed ligand binding sites labels and feature values. After examining the results, we can decide which features could be potentially interesting for the ligand binding sites prediction. Finally, we can train new P2Rank models with these new features and see if the performance has improved.


The analysis pipeline covers most steps of the process, from downloading the files from databases, to computing the statistical analysis of individual features. The only needed input is a dataset file with listed protein identifiers. Moreover, there are two scripts that further extend the pipeline and can be used to train and evaluate P2Rank models with new features. The structure of the pipeline is depicted in Diagram~\ref{fig:diagram}. The details about individual parts are described in this Chapter.


\begin{figure}[!h]\centering
\includegraphics[width=140mm]{../img/pipelineDiagram.pdf}
\caption{Diagram of the pipeline structure.}
\label{fig:diagram}
\end{figure}

The pipeline is implemented in Python, utilizing several Python packages, such as BioPython \cite{biopython}, NumPy \cite{numpy} or SciPy \cite{scipy}. BioPython is an open-source collection of Python tools for computational biology and it was very useful in this work, especially for parsing PDB and FASTA files.

The pipeline comprises a set of Python scripts which are connected together by the main script \texttt{analysis\_pipeline.py}. The main script should be used to run the pipeline. It defines the user API, parses and checks arguments, takes care of logging and runs individual parts of the pipeline. See \url{https://github.com/katebrich/LBS_analysis_pipeline} for more details about options, examples of usage, setup, requirements, input and output.

The features can be defined in `config file'. It is a file in JSON format that lists names of features, their type (binary, categorical, ordinal or continuous) and a path to the class with implementation. Custom config file with user-defined features or with subsets of features can be created and passed as argument \texttt{-c new\_config\_path}.

TODO usage a setup v attachments

The information about versions of used software and databases can be found in Appendix TODO. 


\section{Dataset file}
Dataset file is a mandatory input for the pipeline. It is a plain-text file with each row representing one structure. It has several columns separated by whitespace. The first two columns are mandatory and they contain PDB ID and chain ID (pipeline can only work with single-chain structures). The third column is optional and it can define a comma-separated list of ligands which will be used for the ligand binding sites computation.

\section{FASTA and PDB download}

For each structure, FASTA and PDB files are downloaded from PDBe \cite{pdbe}, via Entry-based REST API \cite{pdbe_restapi}, which is one of the possibilities to access large amouts of data about individual PDB entries programatically.

\section{Residue mappings}

The residue-level mappings are needed for cross-referencing the protein tertiary structure with the sequence annotations and UniProt \cite{uniprot} records. The main reason is that the PDB entry may cover only a segment of the full-length protein and the segment does not have to be continuous.

Furthermore, the numbering of residues in the PDB file can differ from the primary sequence numbering. The residues in the PDB file are assigned identifiers by the author, in order to match the identification used in the publication. The identifier of the residue is composed of two parts. The first one is a residue number, and the second one, called `insertion code', is usually a character and it is left empty for most residues. Typically, it is used to label insertions relative to the reference sequence. The author can assign the numbers how he or she desires; they do not have to start with one or zero, do not have to be in consecutive order and can even be negative.

For those reasons, the residue-level mappings are downloaded from PDBe REST API \cite{pdbe_restapi} early in the pipeline process, they are cached in files and can be used whenever sequence annotations need to be mapped on the structure described in the PDB file.

For the cross-referencing between protein structures and protein sequences in UniProtKB \cite{uniprot}, we used UniProt segments mapping implemented in PDBe REST API. The implementation is based on SIFTS \cite{sifts}, a resource for the transfer of annotations between protein structure and sequence. The mappings are assigned by the SIFTS process with the UniProt sequence as reference and thus, the output is a set of segments that reflects discontinuities in the UniProt sequence.

There is a presumable issue with UniProt segments - sometimes the returned segment does not have the same length in UniProt coordinates and PDB coordinates. We reported it and it was recognized as a bug, but unfortunately has not been corrected before running the experiments in this work; however, it is very rare and it concerns only a few structures; those were removed from the datasets. 

\section{Ligand binding sites}

Each residue is assigned a label \textit{non-binding/binding} (0/1) according to positions of ligands in the PDB file. A residue is labeled as \textit{binding} if it has at least one non-hydrogen surface atom within distance 4.0 {\AA} of a non-hydrogen atom of any ligand. The distance 4.0 {\AA} can be changed in the pipeline with \texttt{-l} argument. This ligand-based definition of binding sites was used in previous large-scale study exploring the composition of binding sites \cite{lbscomposition}.

Only residues on the surface of the protein were taken into consideration in the analysis. The non-surface residues cannot be binding anyway, and excluding them decreases the imbalance between binding and non-binding residues counts. Furthermore, excluding the inner residues helps to reduce potential influence of difference of feature values in surface vs. non-surface residues. For example, inner residues tend to be more hydrophobic in general. Thus, binding sites could seem to be more hydrophilic than non-binding sites, but it would not be clear whether it is not simply the effect of being on the surface of the protein.

To decide which residues are located on the surface, solvent-accessible surface area of each residue was computed. We defined the surface residues as residues that have less than 5\% of their surface accessible to the solvent. This cut-off was proposed by Miller \textit{et al.} \cite{sasaCutoff} and used in other studies\cite{jones,lbscomposition}. 

The solvent-accessible surface was computed using \texttt{Bio.PDB.SASA} module in BioPython \cite{sasa}. It implements Shrake \& Rupley algorithm \cite{shrake} which uses a sphere of particular radius to probe the surface of the protein. It can be imagined as `rolling a ball' along the surface (see Figure~\ref{fig:sasa}). The smaller the sphere radius, the more surface details it can detect. For this work, we used the default radius of 1.4 {\AA}, which approximates the radius of a water molecule.

\begin{figure}[!htbp]\centering
\includegraphics[width=100mm]{../img/sasa.png}
\caption[Illustration of the solvent accessible surface]{Illustration of the solvent accessible surface. It was created by rolling the probe (in blue) along the molecule surface and tracing the center of the probe. Retrieved 02-01-2020 from \url{https://commons.wikimedia.org/wiki/File:Surfacetype_Solvent-Accessible.png}}
\label{fig:sasa}
\end{figure}

\section{Features}

This section describes all implemented features and provides information on how to add user-defined features. The features names and types are summarized in Table~\ref{tab:features}

The pipeline can run only with a subset of implemented features, by listing them and passing as argument (e.g. \texttt{-f hydropathy,aromaticity}). If argument \texttt{-f} is not stated, all features defined in the config file are computed.

The individual features are described in detail in following sections, categorized by the resource that was used for their retrieval.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Name                 & Type        & Source                \\ \midrule
PTM                  & binary      & UniProtKB             \\
lipidation           & binary      & UniProtKB             \\
glycosylation        & binary      & UniProtKB             \\
mod\_res             & binary      & UniProtKB             \\
disulfid             & binary      & UniProtKB             \\
non\_standard        & binary      & UniProtKB             \\
sec\_str             & categorical & UniProtKB             \\
helix                & binary      & UniProtKB             \\
turn                 & binary      & UniProtKB             \\
strand               & binary      & UniProtKB             \\
natural\_variant     & binary      & UniProtKB             \\
variation            & binary      & UniProtKB             \\
compbias             & binary      & UniProtKB             \\
pdbekb\_conservation & ordinal     & PDBe-KB               \\
dynamine             & continuous  & PDBe-KB               \\
efoldmine            & continuous  & PDBe-KB               \\
depth                & continuous  & PDBe-KB               \\
bfactor              & continuous  & PDBe-KB               \\
exposure\_CN         & continuous  & PDB                   \\
HSE\_up              & continuous  & PDB                   \\
HSE\_down            & continuous  & PDB                   \\
phi\_angle           & continuous  & PDBe                  \\
psi\_angle           & continuous  & PDBe                  \\
cis\_peptide         & binary      & PDBe                  \\
aa                   & categorical & FASTA                 \\
hydropathy           & ordinal     & FASTA                 \\
mol\_weight          & ordinal     & FASTA                 \\
polarity             & categorical & FASTA                 \\
charge               & binary      & FASTA                 \\
aromaticity          & binary      & FASTA                 \\
H\_bond\_atoms       & ordinal     & FASTA                 \\
mobiDB               & continuous  & MobiDB                \\
conservation         & continuous  & P2Rank \\ \bottomrule
\end{tabular}
\caption{Summary of analysed features.}
\label{tab:features}
\end{table}

\subsection{UniProtKB}

The UniProt Knowledgebase (UniProtKB) \cite{uniprot} is a large database of well-annotated protein sequence data. It tries to achieve the minimal redundancy of proteomes and it provides detailed, accurate and consistent annotations of the sequences.

Sequence annotations (called `features') are available for every UniProtKB entry. They describe interesting sites and regions on the protein sequence and every feature has an associated description with available evidence, source and related publications. The features are arranged in a well-organized manner on the UniProt website \cite{uniprot_web}, in so called `Features viewer' with many overlapping tracks for different features. Nonetheless, for the purpose of this work, the best way to obtain the features was via the Proteins REST API \cite{proteins_api}. It provides the interface to  access the sequence annotation data as well as mapped variation data programmatically. The API is available at (\url{http://www.ebi.ac.uk/proteins/api/doc}).

Features are classified into eight categories which are further subdivided into types. For example, the category `STRUCTURAL' comprises the types `HELIX', `TURN' and `STRAND'.

The types and categories that were chosen as potentially relevant for ligand binding sites prediction are described below.


\subsubsection{PTM}
Post-translational modifications are covalent chemical modifications of polypeptide chains after translation, usually modifying the functional group of the standard amino acids, or introducing a new group. They extend the set of the 20 standard amino acids and they can be important for the function of many proteins,as they can alter the interactions with other proteins, localization, activity, signal transduction, cell-cell interactions and other properties. Their enrichment in binding sites is very interesting to examine.

Three UniProtKB feature types were analysed: lipidation, glycosylation and type `MOD\_RES' which comprises phosphorylation, methylation, acetylation, amidation, formation of pyrrolidone carboxylic acid, isomerization, hydroxylation, sulfation, flavin-binding, cysteine oxidation and nitrosylation. Only experimentally determined modification sites are annotated, and they are further propagated to related orthologs when specific criteria are met \cite{mod_res}.

Since lipidaton and glycosylation data were very sparse (e.g. there were only 15 lipidation sites in the whole holo4k dataset composed of 3973 proteins), the fourth feature called `PTM' including all three types was added to the analysis.

\subsubsection{Disulfide bonds}
Another type of post-translational modifications are disulfide bonds formed between two cysteine residues. Both intrachain and interchain bonds are annotated by \mbox{UniProtKB}. The disulfide bonds may be either experimentally determined or predicted (occuring in specific protein families) \cite{disulfid}.

\subsubsection{Non-standard residues}
Describes the occurence of non-standard amino acids (selenocysteine and pyrrolysine). There must be experimental evidence for this occurence; however, it can be propagated to close homologs \cite{non_std}.

\subsubsection{Secondary structure}
This feature category annotates three types of secondary structures: helices, beta sheets and hydrogen-bonded turns. Residues not belonging to any of the classes are in a random-coil structure. The `helix' class comprises alpha-helices, pi-helices and 3\textsubscript{10} helices.

The secondary structure assignment is made by DSSP algorithm \cite{dssp} based on the coordinate data sets extracted from the Protein Data Bank (PDB). They are neither predicted computationally, nor propagated to related species \cite{sec_str}.

\subsubsection{Natural variant}
This feature includes naturally occuring polymorphisms, variations between strains or RNA editing events \cite{natural_variant}.

\subsubsection{Variation}
\textit{Variation service} is a utility that can retrieve variation data from UniProtKB. The variants are either extracted from the scientific literature and manually reviewed, or mapped from large scale studies, such as 1000 Genomes \cite{1000genomes}, COSMIC \cite{cosmic}, ClinVar \cite{clinvar} or ExAC \cite{exac}. The Proteins REST API provides various options for variants retrieval, such as to filter by the consequence type, associated disease name, cross reference database type (e.g. ClinVar) or by the source type \cite{proteins_api}.


\subsubsection{Compositional bias}
The regions of compositional bias are parts of the polypeptide chain where some of the amino acids are over-represented, not following the standard frequencies. The regions can be enriched in one or more different amino acids \cite{compbias}.



\subsection{PDBe-KB}
PDBe-KB (Protein Data Bank in Europe - Knowledge Base) \cite{pdbekb} is managed by the PDBe team at the European Bioinformatics Institute. It is a collaborative resource that aims to bring together the annotations from various sources and to show the macromolecular structures in broader biological context. 

One drawback of PDB is that every page represents only one entry that is based on a single experiment. There may be several PDB entries for the full-length protein, each covering only a segment of it. Nevertheless, the entries for the same protein are not interconnected. PDBe-KB has developed the \textit{aggregated views of proteins}, displaying an overview of all the data related to the full-length protein defined by the UniProtKB accession.

The structures from the PDB are extensively used by scientific software and other resources. There exist many valuable annotations, such as ligand binding sites, post-translational modification sites, molecular channels or effects of mutations, that are created outside of the PDB. The problem is that the data is fragmented and therefore it would require immense effort of a researcher to collect and make use of all available data for a structure of interest.

The aggregated views of proteins integrates the annotations from \textit{PDBe-KB partners}, collaborating scientific software developers. It facilitates the retrieval of these annotations with a uniform data access mechanism (via FTP or REST API). The project is called `FunPDBe'. A common data exchange scheme was defined to facilitate the transfer of data \cite{pdbekb}.

The use of PDBe-KB was difficult because of the lack of documentation and a few bugs that were encountered during this work (some of them corrected by now after pointing them out). However, it is understandable since it was launched only two years ago and the constant improvements are done since then.

\subsubsection{Conservation}
PDBe-KB provides pre-calculated residue-level conservation scores, obtained by a pipeline described by Jakubec et al. \cite{3dpatch}.

The values of the score are integers ranging from 0 to 9, with 9 being the most conserved. Since scores higher than 4 were very sparse and the feature would not meet the assumptions of the Chi-squared test (as described below), the scores 4 and higher were merged into one category (4). This does not deteriorate the prediction nor the hypothesis test, as vast majority (over 95\%) of non-binding residues were scored 1 and lower.

\subsubsection{DynaMine}
DynaMine \cite{dynamine} was developed by the Bio2Byte group \cite{bio2byte} and it is one of the PDBe-KB partner resources. It provides the annotations of the backbone dynamics predicted only from the FASTA sequence. DynaMine predicts backbone flexibility at the residue-level, using a linear regression model trained on a large dataset of curated NMR chemical shifts extracted from the Biological Magnetic Resonance Data Bank \cite{bmrb}. The predictor estimates the value of the `order parameter' (S\textsuperscript{2}) which is related to the rotational freedom of the N-H bond vector of the backbone. The values range from 0 (highly dynamic) to 1 (complete order).

\subsubsection{EFoldMine}
EFoldMine \cite{efoldmine} tool comes from the same group as DynaMine. It is a predictor of the early folding regions of proteins. It makes predictions at the residue-level derived only from the FASTA sequence. Internally it uses dynamics predictions and secondary structure propensities as features and the linear regression model is trained on data from NMR pulsed labelling experiments. Unfortunatelly, the early stages of protein folding are not understood very well so far and experimental data is very difficult to obtain. The predictor was trained on the dataset of only 30 proteins and its performance is quite poor.

\subsubsection{Depth}
Depth \cite{depth} is a webserver that can measure residue burial within the protein. It is able to find small cavities in proteins and could be used as a ligand-binding sites predictor as such. The residue depth values are computed from the input PDB file.

The algorithm places input 3D structure in the box of model water, each residue with at least two hydration shells around itself. The water molecules in cavities are removed: the algorithm removes the water molecule if there are less than a given number of water molecules in its spherical volume of given size. The minimum number of neighbouring molecules and the spherical volume can be defined by the user. The removal is iterated until there are no more cavity waters. Residue depth is then computed as the distance to the closest water molecule.


\subsection{PDB}

The following features can be computed or obtained directly from the PDB file.

\subsubsection{B factor}

The B factor, also called the Debye-Waller factor or the temperature factor, describes "the attenuation of X-ray or neutron scattering caused by thermal motion" \cite{bfactor}. It can be used to identify and interpret flexibility of proteins, supposing that high B factors are indicators of higher flexibility, whereas atoms with low B factors generally belong to the well-ordered parts of the structure. B factors can be also view as indicators of the relative vibrational motion of atoms in a protein \cite{bfactor}.

The values can be obtained directly from the PDB files: each ATOM record of a X-ray structure (except for hydrogens) deposited in PDB contains B factor value for the atom. B factor for a residue was computed be averaging B factors of all its atoms. 

\subsubsection{Contact number exposure}

Contact number (CN) is a simple solvent exposure measure that can be computed directly from the 3D structure. The CN value for residue is number of C$\alpha$ atoms within a sphere of chosen radius around the C$\alpha$ of that residue \cite{cn}.

The implementation in BioPython module \texttt{Bio.PDB.HSExposure} was used for computation, with default sphere radius 12 {\AA}.


\subsubsection{Half sphere exposure}

Half sphere exposure (HSE) is a solvent exposure measure introduced by Hamelryck (2005) \cite{hse}. The CN sphere (defined above) around the C$\alpha$ atom is split in two halves by the plane perpendicular to the C$\alpha$-C$\beta$ vector, going through the C$\alpha$, as illustrated in Figure~\ref{fig:hse}. Two different measures are obtained, HSE-up, which is number of C$\alpha$ in `upper' half sphere (containing C$\beta$), and HSE-down, number of C$\alpha$ in the opposite sphere.

Class \texttt{HSExposureCB} from BioPython module \texttt{Bio.PDB.HSExposure} with default sphere radius 12 {\AA} was used. 

\begin{figure}[!h]\centering
\includegraphics[width=80mm]{../img/hse.png}
\caption[Half sphere exposure]{Half sphere exposure. Retrieved 02-01-2020 from \url{https://en.wikipedia.org/wiki/File:HSECa.png}}
\label{fig:hse}
\end{figure}


\subsubsection{Phi and psi angles}

Three dihedral angles of a popypeptide backbone phi ($\phi$), psi ($\psi$) and omega ($\omega$) are depicted on the Figure~\ref{fig:torsion}. While the $\omega$ angle is restricted due to the planar character of the peptide bond, the $\phi$ and $\psi$ angles have high rotational freedom around the N-C$\alpha$ ($\phi$ torsion) or C$\alpha$-C ($\psi$ torsion) bonds. The Ramachandran plot provides good visualization of the whole $\phi$\\$\psi$ space \cite{ramachandran}.
The angles sizes can be computed directly from the PDBe; for the purpose of this work, they were obtained from PDBe via REST API.

\begin{figure}[!h]\centering
\includegraphics[width=120mm]{../img/torsion.png}
\caption[Polypeptide torsion angles phi, psi and omega]{Polypeptide torsion angles phi, psi and omega. Retrieved 02-01-2020 from \url{https://www.researchgate.net/figure/Backbone-torsion-angles-of-a-prototypical-amino-acid-building-block-embedded-in-a-peptide_fig2_284713304}}
\label{fig:torsion}
\end{figure}

\subsubsection{Cis peptide}

The majority of protein bonds is found with torsion angle $\omega$ close to 180$^\circ$, in so-called \textit{trans} conformation. The \textit{cis} isomer, having $\omega$ close to 0$^\circ$, is rather rare. The \textit{cis-trans} isomeration is involved in some biological processes, such as protein folding or membrane binding \cite{cispeptide}.

The residues with \textit{trans} bond are obtained from PDBe via REST API.


\subsection{FASTA}

There are features that can be derived directly from the FASTA sequence. Every amino acid is assigned a value and the feature values are obtained according to the FASTA file. These features are:

\begin{itemize}
\item \textbf{Amino acid} - Categorical feature which is simply the amino acid letter.
\item \textbf{Hydropathy} - The values of hydropathy index proposed by J. Kyte and R. F. Doolittle \cite{kyte}. It takes into consideration hydrophilic and hydrophobic properties of the 20 amino acid side chains. It is based on experimental observations derived from the literature. It ranges from -4.5 (Arg) to 4.5 (Ile) and the larger the number is, the more hydrophobic the amino acid.
\item \textbf{Molecular weight} - Residue mass in Daltons.
\item \textbf{Polarity} - Classification of amino acids according to the side chain - categories Polar, Nonpolar and Polar uncharged.
\item \textbf{Charge} - Binary feature indicating whether the side group is charged in physiological pH.
\item \textbf{Aromaticity} - Binary feature labeling residues that contain aromatic ring.
\item \textbf{Hydrogen bond atoms} - Number of atoms of the side chain that are either hydrogen donor or hydrogen acceptor.
\end{itemize}

The biochemical properties of amino acids (all features above except hydropathy) were obtained from Biochemistry (Voet, 2010 \cite{voet}).

\subsection{Other resources}

\subsubsection{MobiDB}

MobiDB is a database of protein disorder and mobility annotations. It provides annotations and predictions for intrinsically disordered (ID) proteins. MobiDB-lite is a method for highly specific predictions of long (at least 20 residues) disorders. It is a consensus-based prediction, combining results of eight different predictors
\cite{mobidb}. It has been integrated into the MobiDB and the web server provides programmatic access to retrieve single entries via REST API \cite{mobidbApi}.

\subsubsection{P2Rank} \label{s:conservation}

The sequence conservation scores for the feature \texttt{conservation} are computed through Conservation pipeline \cite{conservation} implemented for P2Rank. The conservation scores are computed from multiple sequence alignment (MSA) using the Jensen-Shannon divergence model \cite{jensen}. The pipeline searches for sequences similar to the query sequence in several databases. The details of the implementation were described by Jendele \textit{et al.} \cite{prankweb}.
The pipeline runs on local computer and needs to have SwissProt, UniRef90 and TrEMBL databases downloaded locally.

\subsection{User-defined features}

There are two ways to run the statistical analysis with user-defined features. The first, more time-demanding way is to compute all requited inputs (i.e. feature values and ligand binding sites labels) outside of the pipeline, and then run only the analysis task (with argument \texttt{-tasks A}). More details about the usage and a few  examples are described in the README file in the GitHub repository of the project (\url{https://github.com/katebrich/LBS_analysis_pipeline}).

Another, more straightforward possibility is to implement the new feature directly inside the pipeline. Two steps need to be made:

\begin{itemize}
\item to add the feature to the config file. The class with feature implementation is loaded dynamically according to the feature name, for the easier definition of new features,
\item to implement class with method \texttt{get\_values} which computes the feature values and returns them in the required format.
\end{itemize}

\section{Statistical analysis}

To find the features which are possibly important for prediction of protein-ligand binding sites, statistical analysis has the crucial role. This section describes the method that was used to analyse the statistical significance of the features and to distinguish the ones that stand out in the known protein-ligand binding sites.

In this work, the problem is seen as a hypothesis testing problem. Two populations will be compared: we take values of a feature for all the residues across all the proteins in the dataset and then compare the values associated with the binding residues and non-binding residues.

The \textit{null hypothesis} and the \textit{alternative hypothesis}, denoted by $H_{0}$ and $H_{1}$, respectively, will be tested:

\begin{itemize}
\item \textbf{$H_{0}$} - The feature values in binding sites do not significantly differ from the values in non-binding sites.
\item \textbf{$H_{1}$} - There is a significant difference of feature values in binding sites and non-binding sites.
\end{itemize}

To decide which one of two complementary hypotheses is true, we employ a suitable \textit{hypothesis test}. Welch's test and Chi-squared test of indepence, both described below in more detail, will be used according to the feature type (binary, categorical or continuous). 

As one may expect, the tests are not error-proof and a mistake can be made in the decision of whether to accept or reject the null hypothesis. There are two types of errors in hypothesis testing, commonly known as \textit{Type I error} and \textit{Type II error}. The test has made a Type I error if it incorrectly rejects a true null hypothesis. If, on the other hand, a null hypothesis is accepted and it is not true, a Type II error has been made. Both situations are depicted in the Table~\ref{tab:hypothesis_testing_errors}. The ideal test would have both error probabilities equal to zero. Nevertheless, in most cases it is not possible to make both error probabilities arbitrarily small for a fixed sample size \cite{casella}.

\begin{table}[!htbp]
\centering
\renewcommand{\arraystretch}{2.5}
\begin{tabular}{l|l|c|c|}
\multicolumn{2}{c}{}&\multicolumn{2}{c}{\begin{large}Prediction \end{large}}\\
\cline{3-4}
\multicolumn{2}{c|}{}&Accept $H_{0}$&Reject $H_{0}$\\
\cline{2-4}
\multirow{2}{*}{\begin{large}Truth\end{large}}& \textbf{$H_{0}$} & \shortstack{Correct\\(true positive)} & \shortstack{\textbf{Type I error}\\(false positive)}\\
\cline{2-4}
& \textbf{$H_{1}$} & \shortstack{\textbf{Type II error}\\(false negative)} & \shortstack{Correct\\(true negative)} \\
\cline{2-4}
\end{tabular}
\caption{Type I and II Error in hypothesis testing.}
\label{tab:hypothesis_testing_errors}
\end{table}

To control statistical significance of the result, we define a \textit{significance level}, a constant denoted by $\alpha$. It represents the probability  of making a Type I error, in other words, the probability that the study rejects the null hypothesis when it is true. The typical choices in practice are $\alpha =$ 0.01, 0.05 or 0.10 \cite{casella}. One should be aware that by fixing the significance level of the test, the experimenter is controlling only the Type I error probabilities. The probability of the Type II error is subject to factors such as the accuracy and completeness of the data and most importantly, the true effect size \cite{sham_purcell}. Our choice will be $\alpha =$ 0.05.

The \textit{P value} is reported as a result of the statistical test. The P value is the probability that, under the assumption the null hypothesis is true, we observe the same or greater difference between groups. Smaller values of $p(X)$ give stronger evidence for rejecting the null hypothesis. The null hypothesis is rejected when $p(X) \leq \alpha$. P value gives an idea of how strongly the data contradict the null hypothesis; furthermore, it allows other researchers to make a decision according to the significance level of their choice \cite{pvalue, sham_purcell, lehmann}.

\subsection{Implementation}

The following sections rationalize the choice of Welch's and chi-squared test. The imlementation of these tests in module \texttt{scipy.stats} from the SciPy Python library \cite{scipy} was used in the pipeline; namely \texttt{ttest\_ind} with \texttt{equal\_var=False} to perform Welch's test, and \texttt{chi2\_contingency} for chi-squared test of independence.

By default, the pipeline computes the analysis for all the residues across all proteins in dataset. Neverthless, random sampling (without repetition) can be performed by specifying the sample size with argument \texttt{-s}. It is also possible to run more iterations of random sampling (argument \texttt{-i}). In this case, mean P values will be reported in summary. Individual P values from all iterations will be reported in separate files for each feature. Another possibility is to balance the number of binding and non-binding sites with argument \texttt{-b}. Same number of binding and non-binding residues will be sampled in that case.

The significance level is 0.05 by default and can be changed with \texttt{-a} argument.

The output of the whole analysis pipeline are folders with results for each feature, as well as several summary files:

\begin{itemize}
\item \textbf{p\_values\_means.csv} - Averaged P values obtained from all iterations.
\item \textbf{p\_values.csv} - List of P values from all iterations for all features. P values for individual features are also included in the results folder for each feature.
\item \textbf{cohens\_d.csv} - Effect sizes for continuous features (as described below).
\item \textbf{cohens\_w.csv} - Effect sizes for binary, ordinal and categorical features.
\item \textbf{p\_vals\_perc.csv} - Summary of how many percent of iterations had P value below given significance level $\alpha$ for each feature.
\item \textbf{means\_difference.csv} - Summary only for continuous features. Lists the differences of means and variances in both populations (all binding vs. all non-binding).
\item \textbf{binding\_ratios.csv} - binding/non-binding sites ratios for all rows in the dataset.
\item \textbf{errors.txt} - Lists features that ended up with an error. This is often caused by lack of data (e.g. the sample size is bigger than number of rows or the data for a categorical feature are too sparse to meet the assumptions of Chi-squared test). Detailed information about the errors can be found in the log file.
\end{itemize}

The results folder for each feature contain file \texttt{pairs.txt} with paired ligand binding sites values and feature values, detailed information about all iterations, and various histograms and plots (according to the type of feature). 


\subsection{Welch's test} \label{s:welchs}

Welch's unequal variances t-test, or Welch's test in short, is a two-sample hypothesis test used to decide whether two populations have different central tendencies (means or medians). The decision is made based on the samples from the two populations. It is a more robust alteration of the widely-used Student's t-test \cite{welch}.

Both Student's and Welch's t-test assume that the two examined populations follow a normal distribution \cite{welch}. Nevertheless, when testing for the equality of means of `large enough samples', the normality assumption can be violated thanks to the large sample theory and the Central Limit Theorem \cite{lehmann}. It has been shown in previous studies that for large samples, the statistical significance level is protected not only for normally distributed data, but also for many non-normal distributions; moreover, in case of Welch's test, this is true even for unequal variances \cite{zimmerman_zumbo_1993, zumbo_coulombe_1997, lumley}. According to  Lehmann and Romano \cite{lehmann}, the Type II error is also relatively insensitive to non-normality. Many articles and textbooks mention that when the sample sizes are small, nonparametric tests (i.e. tests that do not assume a specific distribution) such as the Mann-Whitney test \cite{mann} should be considered as an alternative to t-tests.
However, t-tests become superior when sample sizes increase \cite{zimmerman1998, lumley}. The simulations made by Lumley \textit{et al.} \cite{lumley} show that `sufficiently large sample size' means under 100 in most cases. Even for extremely non-normal data, the sufficient size is at most 500. This suggests that the choice of Welch's test is legitimate for this work.

The problem of the Student's t-test is that it performs badly when the variances of the two compared populations are unequal. Both Type I and Type II errors are negatively affected by violation of the equal variances assumption. The unequal variances can be less problematic if sample sizes are similar, but in practice, that is not always the case \cite{ruxton}.

Unlike Student's t-test, Welch's test does not assume equal variances of the populations. It performs well when the samples have unequal variances; furthermore, it can be used even when the samples have unequal sizes \cite{derrick}.

Some researchers tend to pre-test for variance equality by a preliminary test of variances (such as Levene's \cite{levene} or Brown-Forsythe test \cite{brown}) and then choose whether to use Student's or Welch's t-test. However, although this approach persists in some textbooks and software packages, it is not recommended by statisticians. As a preliminary test itself is subject to Type I and II errors, this two-stage procedure would not protect the significance level and could lead to incorrect decisions. One should be aware of the fact that even if the test suggested that the samples variances are nearly equal, it would not mean that the whole population variances could not differ to a larger extent \cite{zimmerman}. Some researchers may try to make the significance level of a preliminary test more strict, so that they could be more confident about the choice of the subsequent test; however, as the significance level decreases, the performance of the compound test paradoxically gets worse. According to Zimmerman \cite{zimmerman}, ``a higher Type I error rate of the preliminary test actually improves the performance of the compound test'' \cite{zimmerman}. This suggests that using the preliminary test is not correct in principle.

Welch's test should be used whenever the researcher is not sure that the variances are truly equal. Ruxton \cite{ruxton} even suggests the routine use of Welch's test. When the sample sizes and variances are equal, both tests perform similarly. When dealing with unequal variances and unequal sample sizes, Welch's test is more robust than Student's t-test and the Type I error rate does not deviate far from the nominal value \cite{derrick}. Hence, Welch's test can be applied without any significant disadvantages to Student's t-test.

For all the reasons stated above, Welch's test seems to be the best choice for the purpose of this study. It has the best combination of performance and ease of use, the calculation is straightforward and it is available in commonly used statistics packages. This test will be used for continuous features.

\subsection{Chi-squared ($\chi^{2}$) test of independence}

A different kind of tests will be needed for the analysis of categorical and binary features. In this section, the $\chi^{2}$ test will be compared to another well-known test for the analysis of data in contingency tables, the Fisher's exact test.

A \textit{contigency table} is a table displayed in a form of a matrix where cells represent a frequency distribution of samples in the categories. An example of a contingency table can be seen in Table~\ref{tab:contingency_table_example}. The sums of frequencies in rows and columns are called \textit{marginal totals}.

\begin{table}[!htbp]
\centering
\renewcommand{\arraystretch}{1.5}
\newcolumntype{s}{>{\columncolor{lightgray}} p{3cm}}
 \begin{tabular}{|c|c|c||c|} 
 \hline
  & Aromatic residue & Non-aromatic residue & Total \\ [0.5ex] 
 \hline
 Binding sites & 1016 & 4654 & 5670 \\ 
 \hline
 Non-binding sites & 4829 & 44545 & 49374 \\
 \hline\hline
 Total & 5845 & 49199 & 55044 \\
 \hline
\end{tabular}
\caption[A $2\times 2$ contingency table for binary feature \texttt{aromaticity}]{A $2\times 2$ contingency table for binary feature \texttt{aromaticity} computed on dataset Chen11.}\label{tab:contingency_table_example}
\end{table}

The null hypothesis assumes independence of the groups; in our case, the assumption is that there is no difference in the proportions of the analysed feature between binding sites and non-binding sites.

Fisher's exact test belongs to a class of so-called \textit{exact tests}; it means that the P value is calculated accurately, not approximately, as is the case of many tests including Welch's test and $\chi^{2}$ test. Fisher's test is mostly used for $2\times 2$ contigency tables, although the principle of the computation can be extended to a general $m\times n$ table \cite{Mehta}. The principle of the test lies in computing the probability of obtaining a table that is more or equally extreme in the departure from the null hypothesis than the analysed table and has identical marginal totals \cite{bland}.

Chi-squared test if independence is able to decide whether the difference between the observed frequencies and the `expected frequencies' is statistically significant. The expected frequencies are computed for every cell using this formula:

\vspace{2mm}
\begin{center}
$\dfrac{row\: total\times column\:total}{grand\:total}$
\end{center}

It can be imagined as the average frequencies we would get in the long run with the same marginal totals, assuming the null hypothesis is true (i.e. there is no association between groups). The result of the test tells how likely are we to observe given data under the assumption of the true null hypothesis \cite{bland}.

The biggest difference between the two mentioned tests is that the chi-squared test is based on a aproximation approach; therefore, it needs a `large enough' sample. W. G. Cochran (1952, 1954) proposed a set of recommendations about the minimum expectations to be used in $\chi^{2}$ tests and about the choice between Fisher's test and $\chi^{2}$ test:

\begin{itemize}
\item \textbf{The 2 x 2 table} - Fisher's exact test should be used whenever the sample size is smaller than 20, or when the sample size is smaller than 40 and if the expected frequency in at least one cell is less than 5. For sample sizes bigger than 40, always use chi-squared test \cite{cochran1952, cochran1954}.
\item \textbf{More than 1 degree of freedom} - Chi-squared test can be used when at most 20\% of cells have the expected frequency less than 5 and no cell have the expected frequencies less than 1 \cite{cochran1954}.
\end{itemize}

 These recommendations are presented in several textbooks and articles as a rule of thumb \cite{cochranRule} and recommended to be used in practice.

As the sample sizes in this work are very large, the number of binding and non-binding sites is unbalanced, and the data for some features can be sparse, chi-squared test should be better choice for both binary and categorical features.

\subsection{Effect size} \label{s:effectsize}
In most cases, the main purpose of research is to estimate actual effects that exist in the real world. Effect size can be understood as ``the degree to which the phenomenon is present in the population" \cite{cohen_book}. In other words, it is the sensitivity of the dependent variable to changes in the independent variable. Effect size complements statistical hypothesis testing and can also help with planning of the sample size \cite{cohen}.

It is considered good practice to report the effect size in addition to statistical tests to provide an objective measure of importance of the results. While statistical significance reflects likeliness of our results, the effect size indicates the practical importance of our findings. For very large samples, even the miniscule effects can become statistically significant, due to the increased power of the statistical test; this is called the \textit{P-value problem} \cite{pvalueproblem}. 

The P-value itself does not have an objective meaning and is not an unambiguous measure of evidence. The sample size hugely influences the significance, and relying only on the P-value can lead to acceptance of the hypothesis of no practical significance. Despite that, this appears to be a common practice. Lin et al. \cite{pvalueproblem} reviewed articles in two leading Information System (IS) journals and reported that 50\% of recent papers with sample sizes over 10,000 were relying on low P-values.

The critical difference between effect size indices and test statistics is that the effect size is not affected by the size of the sample.

There are many measures of effect size, including odds ratio, relative risk or various correlation coefficients (e.g. Pearson's, Spearman's) \cite{effect_sizes}. For the purpose of this work, we will use two effect size indices proposed by Cohen (1992) \cite{cohen} and commonly known as Cohen's \textit{d} and Cohen's \textit{w}.

\subsubsection{Cohen's \textit{d}}

Cohen's \textit{d} is one of the most used effect size measures for continuous data and can be used for all tests of the difference between means of independent samples (e.g. t-tests). It is defined as the difference of the means divided by the pooled standard deviation of the two groups A and B: \cite{cohen_book}

\begin{equation}
d = \frac{\overline{X}_A-\overline{X}_B}{s} ,
\end{equation}

where $\overline{X}_A$ and $\overline{X}_B$ are the means of the two groups and $s$ is the pooled standard deviation for two independent samples defined as:

\begin{equation}
s = \sqrt{\frac{\sum{\left( X_A-\overline{X}_A \right)^2} + \sum{\left( X_B-\overline{X}_B \right)^2}}{n_A+n_B-2}} .
\end{equation}

Cohen \cite{cohen_book} also suggests following interpretation of magnitudes:

\begin{table}[!h] \centering
\begin{tabular}{cc}
\hline
Cohen's \textit{d} & Effect size \\ \hline
0.2                & Small       \\
0.5                & Medium      \\
0.8                & Large       \\ \hline
\end{tabular}
\end{table}

These cutoffs, defined for most effect size measures, provide a good basis for interpreting the effect sizes; however, they tend to be misused and remain controversial practice \cite{effect_sizes}. Even Cohen himself warned about ``many dangers'' emerging from the use of such arbitrary categories. Therefore, these three categories can serve as a guideline for interpretation, but should not be used and trusted blindly.


\subsubsection{Cohen's \textit{w}}

Cohen's \textit{w} is an effect size index used for chi-squared tests. It is defined as

\begin{equation}
w = \sqrt{\sum\limits_{i=1}^k {\frac{\left( P_{1i}-P_{0i} \right)^2}{P_{0i}}}} ,
\end{equation}

where $k$ is the number of cells and $P_{0i}$ and $P_{1i}$ are proportions in cell $i$ under the null and alternative hypothesis \cite{cohen_book}.

The interpretation of magnitudes is following:

\begin{table}[!h] \centering
\begin{tabular}{cc}
\hline
Cohen's \textit{w} & Effect size \\ \hline
0.1                & Small       \\
0.3                & Medium      \\
0.5                & Large       \\ \hline
\end{tabular}
\end{table}

\section{P2Rank models training and evaluation}

Two scripts further extend the analysis pipeline and can be used to train P2Rank models with obtained data. Two dataset files are needed as input - one for training and another for evaluation. The process is following:

\begin{itemize}
\item for both datasets, run the whole pipeline twice:
	\begin{itemize}
	\item in the first run, download data, compute mappings, ligand binding sites, features and analysis with default parameters
	\item in the second run, recompute the analysis with random sampling (sample size 500, 1000 iterations)
	\end{itemize}
\item convert both dataset files to the format accepted by P2Rank
\item create .csv files with custom features using previously computed feature values
\item train and evaluate new P2Rank model on given datasets with custom features
\end{itemize}

It is possible either to train one model with all given features at once with script \texttt{pipeline\_P2Rank\_allFeatures.sh}, or to train one model per feature with \texttt{pipeline\_P2Rank\_oneFeature.sh}

TODO odkaz na attachments


