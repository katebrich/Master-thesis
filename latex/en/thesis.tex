%%% The main file. It contains definitions of basic parameters and includes all other parts.

%% Settings for single-side (simplex) printing
% Margins: left 40mm, right 25mm, top and bottom 25mm
% (but beware, LaTeX adds 1in implicitly)
\documentclass[12pt,a4paper]{report}
\setlength\textwidth{145mm}
\setlength\textheight{247mm}
\setlength\oddsidemargin{15mm}
\setlength\evensidemargin{15mm}
\setlength\topmargin{0mm}
\setlength\headsep{0mm}
\setlength\headheight{0mm}
% \openright makes the following text appear on a right-hand page
\let\openright=\clearpage
\linespread{1.25}

%% Settings for two-sided (duplex) printing
% \documentclass[11pt,a4paper,twoside,openright]{report}
% \setlength\textwidth{145mm}
% \setlength\textheight{247mm}
% \setlength\oddsidemargin{14.2mm}
% \setlength\evensidemargin{0mm}
% \setlength\topmargin{0mm}
% \setlength\headsep{0mm}
% \setlength\headheight{0mm}
% \let\openright=\cleardoublepage

%% Character encoding: usually latin2, cp1250 or utf8:
\usepackage[utf8]{inputenc}
\usepackage[svgnames,table]{xcolor}  % typesetting in color
%% Generate PDF/A-2u
\usepackage[a-2u]{pdfx}
%% Prefer Latin Modern fonts
\usepackage{lmodern}

%% Further useful packages (included in most LaTeX distributions)
\usepackage{amsmath}        % extensions for typesetting of math
\usepackage{amsfonts}       % math fonts
\usepackage{amsthm}         % theorems, definitions, etc.
%\usepackage{bbding}         % various symbols (squares, asterisks, scissors, ...)
\usepackage{bm}             % boldface symbols (\bm)
\usepackage{graphicx}       % embedding of pictures
\usepackage{fancyvrb}       % improved verbatim environment
\usepackage[square,sort,comma,numbers]{natbib}         % citation style AUTHOR (YEAR), or AUTHOR [NUMBER]
\usepackage[nottoc]{tocbibind} % makes sure that bibliography and the lists
			    % of figures/tables are included in the table
			    % of contents
\usepackage{dcolumn}        % improved alignment of table columns
\usepackage{booktabs}       % improved horizontal lines in tables
\usepackage{paralist}       % improved enumerate and itemize

\usepackage{multirow}

\usepackage{listings}
\usepackage{subcaption}

\usepackage{setspace}



%%% Basic information on the thesis

% Thesis title in English (exactly as in the formal assignment)
\def\ThesisTitleAJ{Use of residue-level annotations for structural prediction of protein-ligand binding sites}
\def\ThesisTitleCJ{Využití anotací primární struktury pro strukturní predikci protein-ligand aktivních míst}

% Author of the thesis
\def\ThesisAuthor{Kateřina Břicháčková}

% Year when the thesis is submitted
\def\YearSubmitted{2021}

% Name of the department or institute, where the work was officially assigned
% (according to the Organizational Structure of MFF UK in English,
% or a full name of a department outside MFF)
\def\Department{Katedra buněčné biologie}

% Is it a department (katedra), or an institute (ústav)?
\def\DeptType{Department}

% Thesis supervisor: name, surname and titles
\def\Supervisor{RNDr. David Hoksza, Ph.D.}


% Study programme and specialization
\def\StudyProgramme{Bioinformatics}
\def\StudyBranch{Bioinformatics}

\def\DedicationAJ{%

\vspace{10mm}

In the first place, I want to thank to my parents, sister and grandparents, who always believe in me. The biggest thanks go to my partner David for his constant support and love. And I cannot forget Ondra Skácel to whom I dedicate this thesis.

I would like to thank to my supervisor, David Hoksza, for his guidance throughout writing this thesis, and for many helpful comments and advice.
Many thanks to Radoslav Krivák and Petr Škoda for their kindliness and help with P2Rank program.

Computational resources were supplied by the project "e-Infrastruktura CZ" (e-INFRA LM2018140) provided within the program Projects of Large Research, Development and Innovations Infrastructures.


}

% Abstract (recommended length around 80-200 words; this is not a copy of your thesis assignment!)
\def\AbstractAJ{%
The number of experimentally resolved protein structures in the Protein Data Bank has been growing fast in the last 20 years, which motivates the development of many computational tools for protein-ligand binding sites prediction. Binding sites prediction from protein 3D structure has many important applications; it is an essential step in the complex process of rational drug design, it helps to infer the side-effects of drugs, it provides insight into proteins biological functions and it is helpful in many other fields, such as protein-ligand docking and molecular dynamics. As far as we know, there has not been a study that would systematically investigate general properties of known ligand binding sites on a large scale. In this thesis, we examine these properties using existing experimental and predicted residue-level annotations of protein sequence and structure. We present an automated pipeline for statistical analysis of these annotations, based on hypothesis testing and effect size estimation. It is implemented in Python and it is easily extensible by user-defined annotations. The usage is demonstrated on 33 existing annotations and 4 different datasets. The practical significance of the results is tested with P2Rank prediction method. We hope that the results as well as the pipeline could be eventually helpful for improving the performance of the existing binding sites predictors.
}

\def\AbstractCJ{%

V posledních 20 letech se počet experimentálních proteinových struktur v databá\-zi Protein Data Bank rychle zvyšuje, což motivuje vývoj nástrojů pro predikci protein-ligand vazebných míst. Strukturní predikce vazebných míst má mnoho důležitých aplikací; je klíčovým krokem v komplexním procesu návrhu léčiv, pomáhá objevovat vedlejší účinky léčiv, umožňuje chápat biologické funkce proteinů a je využívá se i v mnoha jiných oborech, jako je protein-ligand docking nebo molekulová dynamika. Pokud je autorce známo, dosud nebyla provedena studie, která by systematicky zkoumala obecné vlastnosti známých vazebných míst na velkých datasetech. Tato práce se zaměřuje na analýzu těchto vlastností, s využitím existujících experimentálních i predikovaných anotací primární a terciální struktury proteinu. Je zde představena metoda pro statistickou analýzu těchto anotací, která je založena na testování hypotéz a odhadu velikosti účinku. Metoda je implementována v jazyce Python a lze ji jednoduše rozšířit o nové anotace definované uživatelem. Použití je demonstrováno na 33 existujících anotacích a čtyřech různých datasetech. Praktická významnost výsledků je otestována s pomocí metody P2Rank. Výsledky i samot\-ná metoda pro statistickou analýzu by mohly posléze přispět ke zlepšení úspěšnos\-ti existujících nástrojů pro predikci vazebných míst.

}

% 3 to 5 keywords (recommended), each enclosed in curly braces
\def\KeywordsAJ{%
{binding sites}, {protein-ligand binding sites}, {binding sites prediction}, {P2Rank}, {residue-level annotations}, {3D-based prediction}, {statistical analysis}
}

\def\KeywordsCJ{% 
{vazebná místa}, {protein-ligand vazebná místa}, {predikce vazebných míst}, {P2Rank}, {anotace primární struktury}, {3D-based predikce}, {statistická analýza}
}

%% The hyperref package for clickable links in PDF and also for storing
%% metadata to PDF (including the table of contents).
%% Most settings are pre-set by the pdfx package.
\hypersetup{unicode}
\hypersetup{breaklinks=true}

% Definitions of macros (see description inside)
\include{macros}

% Set level of sections appearing in the contents table
\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{4}

% Title page and various mandatory informational pages
\begin{document}
\include{title}

%%% A page with automatically generated table of contents of the thesis
\tableofcontents

\include{introduction}
\include{lbs_predictors}
\include{methodology}
\include{results}
\include{epilog}

%%% Abbreviations used in the thesis, if any, including their explanation
%%% In mathematical theses, it could be better to move the list of abbreviations to the beginning of the thesis.
\chapwithtoc{List of Abbreviations}
\begin{tabular}{ll}
AA & Amino Acid \\
API & Application Programming Interface \\
AUC & Area Under the Curve \\
CN & Contact Number \\
CNN & Convolutional Neural Networks \\
CPU & Central Processing Unit \\
CSV & Comma-Separated Values \\
FTP & File Transfer Protocol \\
HSE & Half Sphere Exposure \\
ID & Identifier  \\
ID & Intrinsic Disorder \\
IS & Information Systems \\
JSON & JavaScript Object Notation \\
LBS & Ligand Binding Site \\
MCC & Matthews Correlation Coefficient \\
MSA & Multiple Sequence Alignment \\
NMR & Nuclear Magnetic Resonance \\
PDB & Protein Data Bank \\
PTM & Post-Translational Modification \\
REST & Representation State Transfer \\
SAS & Solvent Accessible Surface \\

\end{tabular}


%%% Bibliography
\include{bibliography}

%%% Figures used in the thesis (consider if this is needed)
\listoffigures

%%% Tables used in the thesis (consider if this is needed)
%%% In mathematical theses, it could be better to move the list of tables to the beginning of the thesis.
\listoftables



%%% Attachments to the bachelor thesis, if any. Each attachment must be
%%% referred to at least once from the text of the thesis. Attachments
%%% are numbered.
%%%
%%% The printed version should preferably contain attachments, which can be
%%% read (additional tables and charts, supplementary text, examples of
%%% program output, etc.). The electronic version is more suited for attachments
%%% which will likely be used in an electronic form rather than read (program
%%% source code, data files, interactive charts, etc.). Electronic attachments
%%% should be uploaded to SIS and optionally also included in the thesis on a~CD/DVD.
%%% Allowed file formats are specified in- provision of the rector no. 72/2017.
\appendix
\chapter{Attachments}

\singlespacing

The attachements are following:
\begin{itemize}
\item LBS\_analysis\_pipeline.zip - Source code of the analysis pipeline. Includes config files and input dataset files.
\item P2Rank\_models.zip - Text summary files returned by the P2Rank method. Individual runs and models are not included due to the space limitations, but can be reconstructed from the enclosed data. All parameters are listed in \texttt{params.txt} file.
\item Analysis\_results.zip - Text outputs of the statistical analysis returned by the pipeline. Plots are not included due to the space limitations, but can be reconstructed by running the analysis pipeline with enclosed data.
\item data (only on DVD) - Data computed by the analysis pipeline for all datasets with all filters.
\end{itemize}

Note that by running the analysis pipeline with enclosed input dataset files, the results might not be exactly the same, since the databases and other resources are often updated. Nevertheless, the results should be very similar. To reproduce the same results as in this thesis, the analysis must be computed from the enclosed data.


\section{Analysis pipeline} \label{a:pipeline}
This section describes the setup and usage of the analysis pipeline. For more detailed instructions, examples and explanations of the functionality, see project GitHub repository \url{https://github.com/katebrich/LBS_analysis_pipeline}.

The pipeline was developed and tested on Linux operating system.

\subsection{Setup}
The pipeline requires no installation, only several Python packages installed. For the setup, follow these steps:
\begin{enumerate}
\item Unpack the attached source code:

\texttt{unzip LBS\_analysis\_pipeline.zip}

\item Go into the new folder:

\texttt{cd LBS\_analysis\_pipeline}

\item Required packages can be installed using requirement file:

\texttt{pip install -r requirements.txt}

Alternatively, a virtual environment can be created with \texttt{conda}:

\texttt{conda env create -f environment.yml}

\item Now, it is possible to run the pipeline in the new environment:

\texttt{python3 scripts/source/analysis\_pipeline.py \textbackslash \newline -d data/datasets/test.txt -o output/test}

\item Note! Before usage of the scripts for the P2Rank model training \newline (\texttt{pipeline\_P2Rank\_oneFeature.sh} and \texttt{pipeline\_P2Rank\_allFeatures.sh}), it is required to change the \texttt{P2RANK\_PATH} variable in the scripts according to the P2Rank installation directory.

\end{enumerate}

\subsection{Options and arguments}
\begin{Verbatim}[fontsize=\small]
Usage:
  analysis_pipeline.py 
  	-d DATASET_FILE_PATH
  	-o OUTPUT_DIR_PATH [OPTIONS]... 

Options:
  -d, --dataset                
  	Mandatory; file with listed structures to process.
  -o, --output_dir             
  	Mandatory; root folder. Created if does not exist.
  -t, --tasks                  
  	Default: 'A'. Comma-separated list of tasks to process. 
  	If data are missing in root folder for some task, they are 
  	computed even if their task is not in the list.
  	Possible values: 'D' - download; 'L' - compute ligand binding 
  	sites; 'F' - compute features; 'A' - compute analysis
  -m, --threads                
  	Default: 4. Number of threads.
  -f, --features               
  	Comma-separated list of features. If not provided, all
  	features from config are processed.
  -c, --config_path            
  	Default: file config.json located in the same directory 
  	as this script.
  -l, --lbs_distance_threshold 
  	Default: 4. Binding residues are defined as residues with 
  	at least one non-hydrogen atom in distance at most 
  	lbs_binding_threshold from any ligand.
  -s, --sample_size            
  	Default: 0. Size of random sample for hypothesis tests. 
  	If 0, all rows are taken. Arguments -i and -b are not
  	considered, as this only makes sense for 1 iteration
  	and no balancing.
  -i, --iterations             
  	Default: 1. Number of iterations of hypothesis tests. 
  	Summary files contain averaged results from all the iterations.
  -b, --balance_binding_ratio  
  	Default: False. If false, sample of given size is taken from
  	the whole dataset and binding/nonbinding ratio is not balanced. 
  	If true, the same number of binding rows and nonbinding rows
  	(equal to given sample size) is taken. 
  -p, --draw_plots             
  	Default: True.
  -a, --alpha                  
  	Default: 0.05. Statistical significance level. 
\end{Verbatim}


\subsection{Dataset files}
After unpacking the source code (LBS\_analysis\_pipeline.tar.gz), the dataset files for all used datasets can be found in folder `data/datasets'.


\section{Experiments} \label{a:experiments}

This section describes the procedures used for obtaining the results discussed in this thesis.

\subsection{Statistical analysis} \label{a:experiments1}
The analysis pipeline was run with default parameters:

\begin{Verbatim}[fontsize=\small]
python3 scripts/source/analysis\_pipeline.py \
	-d data/datasets/DATASET_NAME.txt -o output/DATASET_NAME
\end{Verbatim}

where `DATASET\_NAME' was substituted with the name of the dataset file, e.g. `chen11' or `coach420\_filter\_MOAD'.

Note than since conservation scores for \texttt{conservation} feature are computed locally and the computation takes very long time, it was not included directly in the pipeline. The scores were obtained outside of the pipeline by the sequnce conservation pipeline (usage is described at \url{https://github.com/cusbg/sequence-conservation}). The pipeline was then used only for parsing the files and running the analysis.

\subsection{Statistical analysis - random sampling} \label{a:experiments2}
The pipeline analysis was run with following settings to run 1000 iterations of random sampling, each time taking 500 binding and 500 non-binding residues:

\begin{Verbatim}[fontsize=\small]
python3 scripts/source/analysis\_pipeline.py \
	-d data/datasets/DATASET_NAME.txt -o output/DATASET_NAME \
	-s 500 -i 1000 -b true
\end{Verbatim}

\subsection{P2Rank models} \label{a:experiments3}

The P2Rank models training was done using scripts \newline \texttt{pipeline\_P2Rank\_allFeatures.sh} (trains one model with all given features together) and \texttt{pipeline\_P2Rank\_oneFeature.sh} (takes one feature per one model). The usage is the same for both scripts:

\begin{Verbatim}[fontsize=\small]
bash scripts/pipeline_P2Rank_oneFeature.sh \
	-t data/datasets/chen11.txt \
	-e data/datasets/coach420.txt \
	-l 10 -m 4 \
	-f pdbekb_conservation,depth,...(*all desired features here*)
\end{Verbatim}

\subsection{Practical example: comparison of conservation features} \label{a:experiments4}

The procedure was following:
\begin{enumerate}
\item We installed INTAA-conservation tool and downloaded Swiss-Prot \newline database, as described in the project repository (\url{https://github.com/davidjakubec/INTAA-conservation}). Conservation scores for all structures in datasets \textit{chen11} and \textit{coach420} were calculated.
\item Similarly, we calculated conservation scores using P2Rank conservation pipeline. The usage is described at \newline \url{https://github.com/cusbg/sequence-conservation}.
\item New dataset files \texttt{chen11\_conservation.txt} and \newline \texttt{coach420\_conservation.txt} were created by removing the structures with missing data for any of the three features.
\item We created new config file \texttt{config\_conservation.json} with definitions of three conservation features (included in the source code folder).
\item We implemented new feature \texttt{INTAA\_conservation} in class \newline \texttt{Features.Custom.INTAAConservation} (included in the source code).
\item Since conservation scores for \texttt{conservation} and \texttt{INTAA\_conservation} were computed locally and the computation was not included directly in the pipeline, inside the pipeline, these features were implemented so that they read and parse the files obtained by the conservation tools and supply values for individual residues to the pipeline. For this reason, before running the pipeline we must create the output folder (named as the dataset file) and place the pre-computed files into the subfolders named `conservation' and `INTAA\_conservation'.
\item The following command was run to download the PDB files, compute the features, perform the statistical analysis with and without the random sampling, prepare csv files with features for P2Rank, prepare dataset files for P2Rank and train and evaluate three P2Rank models with each conservation feature:

\begin{Verbatim}[fontsize=\small] 
bash scripts/pipeline_P2Rank_oneFeature.sh \ 
-t data/datasets/chen11_conservation.txt \
-e data/datasets/coach420_conservation.txt \
-c scripts/source/config_conservation.json \
-l 10 -m 4 \
-f conservation,INTAA_conservation,pdbekb_conservation
\end{Verbatim}

\end{enumerate}

\section{Used software and databases} \label{a:software}

Following software and dababases were used for the development of the analysis pipeline and running the experiments:

\begin{itemize}
\item Python 3.8.2
\item BioPython 1.76
\item NumPy 1.18.1
\item SciPy 1.4.1
\item Matplotlib 3.1.3
\item P2Rank 2.3-dev1 (development version), downloaded 19.11.2020 \newline from \url{https://github.com/rdk/p2rank}
\item INTAA-conservation - downloaded 20.12.2020 \newline from \url{https://github.com/davidjakubec/INTAA-conservation}
\item Sequence conservation calculator - downloaded 2.11.2020 \newline from \url{https://github.com/cusbg/sequence-conservation}
\item UniProtKB - release 2020\_05
\item PDBe - November 2020
\item PDBe-KB - November 2020
\item MobiBD 4.0.1
\item UniProtKB/SwissProt (used by INTAA-conservation tool) - release 2020\_06
\item UniRef90 (used by Sequence conservation calculator) - release 2020\_05
\item UniProtKB/SwissProt (used by Sequence conservation calculator) \newline - release 2020\_05
\item UniProtKB/TrEMBL (used by Sequence conservation calculator) \newline - release 2020\_05

\end{itemize}


\openright
\end{document}